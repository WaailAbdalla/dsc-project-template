{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "A residential builder based in the USA has had much success on the east coast. They are looking to expand their market into the west coast of the USA, starting in King County, Washington. The residential builder would like to understand what factors influence the price of a house in that area.\n",
    "\n",
    "The aim is to generate designs of a house based on those 5 factors and offer it as an option in King County. \n",
    "\n",
    "The model generated will be based on inference rather than prediction. I want to identify which features have a strong relationship with house price and see their effect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/kc_house_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13240\\2980900483.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/kc_house_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/kc_house_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "df = pd.read_csv('data/kc_house_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types, identify columns for cleaning\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 object columns, Date and sqft_basement\n",
    "# Date column does not affect target variable price. Does not require cleaning\n",
    "# Investigate sqft_basement column\n",
    "\n",
    "df['sqft_basement'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '?' is present. Check how many instances this occurs\n",
    "\n",
    "df['sqft_basement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '?' has 454 instances. Could be a placeholder or does not have a basement\n",
    "# 0.0 has 12,826 instances out of 21,597 . This means 59% of houses do not have a basement\n",
    "# Therefore I will treat '?' as 0.0\n",
    "\n",
    "df['sqft_basement'] = df['sqft_basement'].replace(\"\\?\", 0, regex=True)\n",
    "df['sqft_basement'] = df['sqft_basement'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-check data types\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify NaN values\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate view column\n",
    "\n",
    "df['view'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['view'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View looks to be categorical data (rating system)\n",
    "# Majority of houses do not have a view rating plus small amount of NaN values\n",
    "# Therefore I will treat NaN values as 0.\n",
    "\n",
    "df['view'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate waterfront column\n",
    "\n",
    "df['waterfront'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['waterfront'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfront looks to be categorical data (yes or no)\n",
    "# Majority of houses do not have a waterfront. Makes sense in terms of real world application\n",
    "# Therefore I will treat NaN values as 0.\n",
    "\n",
    "df['waterfront'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate yr_renovated column\n",
    "\n",
    "df['yr_renovated'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yr_renovated'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again yr_renovated looks to be categorical data (years)\n",
    "# Majority have not been renovated. Makes sense in terms of real world application\n",
    "# Therefore I will treat NaN values as 0.\n",
    "\n",
    "df['yr_renovated'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NaN values\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns id and date because they definitely do not affect price\n",
    "# Use a heatmap to visualise correlation between variables\n",
    "\n",
    "df = df.drop(columns=['id', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df.corr(), annot=True, center=0, cmap='mako');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now ordered in a way to see ranking of correlations\n",
    "# This will help in decision making of what variables to consider during transformations\n",
    "\n",
    "price_corr = df.corr()[['price']].sort_values(by='price', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(4, 6))\n",
    "heatmap = sns.heatmap(price_corr, annot=True, cmap='mako')\n",
    "heatmap.set_title('Variables Correlating with Price', fontsize=14);\n",
    "plt.savefig(\"Images/df_price_corr.png\", bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1 (Baseline Model)\n",
    "\n",
    "Now that I am happy with my EDA, I will generate a baseline model. This model will contain all data and no transformations. It will be compared to subsequent iterations to observe the effect of the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'price']\n",
    "categoricals = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade', 'yr_built', 'yr_renovated', 'zipcode']\n",
    "\n",
    "# Log transform and normalize\n",
    "df_cont = df[continuous]\n",
    "\n",
    "for col in categoricals:\n",
    "    df_cont[col] = df[col].astype('category')\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df_cat = pd.get_dummies(df_cont[categoricals], prefix=categoricals, drop_first=True)\n",
    "\n",
    "df_baseline = pd.concat([df_cont, df_cat], axis=1)\n",
    "\n",
    "X = df_baseline.drop('price', axis=1)\n",
    "y = df_baseline['price']\n",
    "\n",
    "X_int = sm.add_constant(X)\n",
    "model = sm.OLS(y,X_int).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 1 Model Comments\n",
    "\n",
    "So far the P-values indicate all independent variables except sqft_basement are statistically significant. That would make sense given that all the features we have chosen would have an effect on the price of the house. The adjusted R-squared is a great value however there are many categorical variables that are not statistically significant leading to high variance.\n",
    "\n",
    "Skew = 2.235 indicates the model is positively skewed\n",
    "\n",
    "Kurtosis = 43.117 indicates this is a leptokurtic curve. Data likely has heavy tails and many outliers\n",
    "\n",
    "However, the aim is to narrow down which independent variables has the strongest effect on the target variable. The builder will then be able to focus their design and budget on those parameters.\n",
    "\n",
    "I will explore each variable and evaluate against the linearity assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions and KDE\n",
    "\n",
    "Visualise the distribution of each variable. If they are not normally distributed, determine the next step to evaluate the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "columns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'price']\n",
    "\n",
    "for i, column in enumerate(columns):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "\n",
    "    sns.histplot(data=df, x=column, ax=axs[row, col])\n",
    "    axs[row, col].set_xlabel(column)\n",
    "    axs[row, col].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"images/dist_it1\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution and KDE comments\n",
    "\n",
    "Normal distribution with positive skew\n",
    "* price\n",
    "* sqft_living\n",
    "* sqft_lot\n",
    "* sqft_above\n",
    "* sqft_basement\n",
    "* long \n",
    "* sqft_living15 \n",
    "\n",
    "Normal distribution with negative skew\n",
    "* lat\n",
    "\n",
    "Not a great distribution. Maybe outliers greatly affecting it\n",
    "* sqft_lot15\n",
    "\n",
    "Many of the histograms also exhibit quite large tails. This is an indication of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Linearity Assumption\n",
    "\n",
    "Identify which variables have a linear relationship with the target variable price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = len(df.columns)\n",
    "num_rows = (num_columns + 2) // 3  # Calculate the number of rows needed to display the subplots\n",
    "\n",
    "fig, axs = plt.subplots(num_rows, 3, sharey=True, figsize=(18, 6 * num_rows))\n",
    "\n",
    "for idx, column in enumerate(df.columns):\n",
    "    row_idx = idx // 3  # Calculate the row index for the subplot\n",
    "    col_idx = idx % 3  # Calculate the column index for the subplot\n",
    "\n",
    "    axs[row_idx, col_idx].scatter(df[column], df['price'])\n",
    "    axs[row_idx, col_idx].set_xlabel(column)\n",
    "    axs[row_idx, col_idx].set_ylabel('price')\n",
    "\n",
    "# Hide empty subplots\n",
    "if num_columns % 3 != 0:\n",
    "    for idx in range(num_columns, num_rows * 3):\n",
    "        row_idx = idx // 3\n",
    "        col_idx = idx % 3\n",
    "        plt.delaxes(axs[row_idx, col_idx])\n",
    "\n",
    "plt.tight_layout()  # Adjust the spacing between subplots\n",
    "\n",
    "plt.savefig(\"images/scat_lin_it1\", bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity Comments\n",
    "\n",
    "Confirmed as categorical data.\n",
    "* bedroom\n",
    "* bathroom\n",
    "* floors\n",
    "* waterfront\n",
    "* view\n",
    "* condition\n",
    "* grade\n",
    "\n",
    "They are clustered but the scatter plot indicate these variables are categorical data.\n",
    "* yr_built\n",
    "* yr_renovated\n",
    "* zipcode\n",
    "\n",
    "Variables that have a strong linear relationship with price. These variables match up well with the ranking in the correlation heat map.\n",
    "* price\n",
    "* sqft_living\n",
    "* sqft_above\n",
    "* sqft_living15\n",
    "\n",
    "This variable has a strong linear relationship if there is a basement present.\n",
    "Otherwise there are many without a basement.\n",
    "* sqft_basement\n",
    "\n",
    "Variables that have a weak linear relationship with price.\n",
    "* sqft_lot\n",
    "* lat\n",
    "* long\n",
    "* sqft_lot15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Normality and Homoscedasticity Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "f = 'price~bedrooms'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'bedrooms': [data.bedrooms.min(), data.bedrooms.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='bedrooms', y='price', ax=ax)\n",
    "ax.plot(X_new['bedrooms'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"bedrooms\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality and homoscedasticity and categorical variables\n",
    "\n",
    "A straightforward concept but these results were produced to highlight the importance of transforming categorical variables.\n",
    "\n",
    "These plots represent a prediction line, the error terms, heteroscedasticity and Q-Q plot. Using categorical variables, the data is not plotted in the way we expect for continuous. However we do know the independent variable has an effect on the price of a house based on the heat map and the P-value in the baseline model. Therefore, all categorical data will need to have dummy variables created in order to observe its relationship with price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the normality and homoscedasticity assumptions on continuous variables\n",
    "\n",
    "data=df\n",
    "f = 'price~sqft_living'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_living': [data.sqft_living.min(), data.sqft_living.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_living', y='price', ax=ax)\n",
    "ax.plot(X_new['sqft_living'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_living\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "plt.savefig(\"images/price_sqft_living_norm_homo_it1\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "f = 'price~sqft_above'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_above': [data.sqft_above.min(), data.sqft_above.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_above', y='price', ax=ax)\n",
    "ax.plot(X_new['sqft_above'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_above\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "f = 'price~sqft_living15'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_living15': [data.sqft_living15.min(), data.sqft_living15.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_living15', y='price', ax=ax)\n",
    "ax.plot(X_new['sqft_living15'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_living15\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "f = 'price~sqft_basement'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_basement': [data.sqft_basement.min(), data.sqft_basement.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_basement', y='price', ax=ax)\n",
    "ax.plot(X_new['sqft_basement'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_basement\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "plt.savefig(\"images/price_sqft_basement_norm_homo_it1\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "f = 'price~sqft_lot'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_lot': [data.sqft_lot.min(), data.sqft_lot.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_lot', y='price', ax=ax)\n",
    "ax.plot(X_new['sqft_lot'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_lot\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "f = 'price~lat'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'lat': [data.lat.min(), data.lat.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='lat', y='price', ax=ax)\n",
    "ax.plot(X_new['lat'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"lat\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "f = 'price~long'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'long': [data.long.min(), data.long.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='long', y='price', ax=ax)\n",
    "ax.plot(X_new['long'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"long\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "plt.savefig(\"images/price_sqft_long_norm_homo_it1\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "f = 'price~sqft_lot15'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_lot15': [data.sqft_lot15.min(), data.sqft_lot15.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_lot15', y='price', ax=ax)\n",
    "ax.plot(X_new['sqft_lot15'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_lot15\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homoscedasticity and Normality Assumption Comments\n",
    "\n",
    "Below are the observations made when checking the linearity assumption with the R-squared value generated from the prediction line plot\n",
    "\n",
    "Strong linear relationship\n",
    "* price (target variable)\n",
    "* sqft_living - 0.49\n",
    "* sqft_above - 0.37\n",
    "* sqft_living15 - 0.34\n",
    "\n",
    "Possible strong linear relationship if there is a basement present\n",
    "* sqft_basement - 0.10\n",
    "\n",
    "Weak linear relationship\n",
    "* sqft_lot - 0.008\n",
    "* lat - 0.09\n",
    "* long - 0.0004\n",
    "* sqft_lot15 - 0.006\n",
    "\n",
    "It is clear what was observed in the linearity assumption matches with the prediction line plot. It would be safe to say that the weak linear relationship variables can be disregarded except for sqft_basement. The low R-squared value might be affected by the high volume of houses without a basement. However if the house has behaviour, the plot behaves similarly to the strong linear relationship variables. It is worth investigating the sqft_basement with the data that includes only basement houses.\n",
    "\n",
    "Therefore, below are the variables to be considered for verifying the homoscedasticity and normality assumption\n",
    "\n",
    "* sqft_living\n",
    "* sqft_above\n",
    "* sqft_living15\n",
    "\n",
    "All variables violate the homoscedasticity assumption. For the normality assumption, all variables can be rejected based on the Q-Q plot. However I have seen previously that these variables are normally distributed. Therefore transformations are required to help normalise the distribution and pass the homoscedasticity and normality assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring sqft_basement without houses that do not have a basement\n",
    "\n",
    "df_sqft_basement = df.iloc[:, [0,11]]\n",
    "df_sqft_basement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sqft_basement.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sqft_basement = df_sqft_basement[df_sqft_basement['sqft_basement'] != 0.0]\n",
    "df_sqft_basement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sqft_basement.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_sqft_basement\n",
    "f = 'price~sqft_basement'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_basement': [data.sqft_basement.min(), data.sqft_basement.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_basement', y='price', ax=ax)\n",
    "ax.plot(X_new['sqft_basement'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_basement\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "plt.savefig(\"images/price_sqft_basement_norm_homo_it1_no_zero\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the houses that did not have a basement increased the R-squared from 0.10 to 0.16. However this indicates a weak relationship with the price and will also be disregarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2\n",
    "\n",
    "Below are the observations made in iteration 1 and how I will address them in iteration 2\n",
    "\n",
    "### From the Linearity Comments in Iteration 1\n",
    "\n",
    "These independent variables are categorical variables with their correlation score from the heat map.\n",
    "* grade - 0.67\n",
    "* bathrooms - 0.53\n",
    "* view - 0.39\n",
    "* bedrooms - 0.31\n",
    "* floors - 0.26\n",
    "* waterfront - 0.26\n",
    "* yr_renovated - 0.12\n",
    "* yr_built - 0.054\n",
    "* condition - 0.036\n",
    "* zipcode - -0.053\n",
    "\n",
    "Based on the correlation score, I will not use the bottom 4 independent variables in this model. They do not have a strong enough correlation with the target variable. Dummy variables will be created so the remaining categorical variables can be used in the regression model.\n",
    "\n",
    "### From Linearity, Homoscedasticity and Normality Comments in Iteration 1\n",
    "\n",
    "It has been shown through verifying the assumptions that these variables do not have a strong enough linear relationship. I will consider dropping these variables especially during the multicollinearity checking phase.\n",
    "\n",
    "* sqft_lot\n",
    "* lat\n",
    "* long\n",
    "* sqft_lot15\n",
    "* sqft_basement\n",
    "\n",
    "The variables below have shown to have a strong relationship with the target variable however violate the linear assumptions. I will use log transformations to improve performance.\n",
    "\n",
    "* sqft_living\n",
    "* sqft_above\n",
    "* sqft_living15\n",
    "\n",
    "Before the transformations are applied, I will address the outliers observed in the normal distributions and scatter plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['bedrooms'], df['price'])\n",
    "plt.xlabel('Bedrooms')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Price vs Bedrooms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for bedrooms indicates an outlier to the far right\n",
    "\n",
    "df['bedrooms'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['bedrooms'] == 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One house has 33 bedrooms and sqft_living of 1620. This is below the mean\n",
    "# It is unlikely this house has 33 bedrooms, possibly a recording error\n",
    "\n",
    "df['bedrooms'] = df['bedrooms'].replace(33, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bedrooms'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several sqft variables have outliers to the right. Could be related\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "axs[0, 0].scatter(df['sqft_living'], df['price'])\n",
    "axs[0, 0].set_xlabel('sqft_living')\n",
    "axs[0, 0].set_ylabel('Price')\n",
    "\n",
    "axs[0, 1].scatter(df['sqft_lot'], df['price'])\n",
    "axs[0, 1].set_xlabel('sqft_lot')\n",
    "axs[0, 1].set_ylabel('Price')\n",
    "\n",
    "axs[0, 2].scatter(df['sqft_above'], df['price'])\n",
    "axs[0, 2].set_xlabel('sqft_above')\n",
    "axs[0, 2].set_ylabel('Price')\n",
    "\n",
    "axs[1, 0].scatter(df['sqft_basement'], df['price'])\n",
    "axs[1, 0].set_xlabel('sqft_basement')\n",
    "axs[1, 0].set_ylabel('Price')\n",
    "\n",
    "axs[1, 1].scatter(df['sqft_lot15'], df['price'])\n",
    "axs[1, 1].set_xlabel('sqft_lot15')\n",
    "axs[1, 1].set_ylabel('Price')\n",
    "\n",
    "axs[1, 2].scatter(df['sqft_living15'], df['price'])\n",
    "axs[1, 2].set_xlabel('sqft_living15')\n",
    "axs[1, 2].set_ylabel('Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "sns.histplot(data=df, x='sqft_living', ax=axs[0, 0])\n",
    "axs[0, 0].set_xlabel('sqft_living')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data=df, x='sqft_lot', ax=axs[0, 1])\n",
    "axs[0, 1].set_xlabel('sqft_lot')\n",
    "axs[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data=df, x='sqft_above', ax=axs[0, 2])\n",
    "axs[0, 2].set_xlabel('sqft_above')\n",
    "axs[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data=df, x='sqft_basement', ax=axs[1, 0])\n",
    "axs[1, 0].set_xlabel('sqft_basement')\n",
    "axs[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data=df, x='sqft_lot15', ax=axs[1, 1])\n",
    "axs[1, 1].set_xlabel('sqft_lot15')\n",
    "axs[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data=df, x='sqft_living15', ax=axs[1, 2])\n",
    "axs[1, 2].set_xlabel('sqft_living15')\n",
    "axs[1, 2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"images/cont_var_hist_it2\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce outliers by reducing data size to 3 standard deviations\n",
    "\n",
    "filter_cols = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_lot15', 'sqft_living15']\n",
    "\n",
    "df1 = df[~df[filter_cols].apply(lambda x: np.abs(x - x.mean()) > 3 * x.std()).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "axs[0, 0].scatter(df1['sqft_living'], df1['price'])\n",
    "axs[0, 0].set_xlabel('sqft_living')\n",
    "axs[0, 0].set_ylabel('Price')\n",
    "\n",
    "axs[0, 1].scatter(df1['sqft_lot'], df1['price'])\n",
    "axs[0, 1].set_xlabel('sqft_lot')\n",
    "axs[0, 1].set_ylabel('Price')\n",
    "\n",
    "axs[0, 2].scatter(df1['sqft_above'], df1['price'])\n",
    "axs[0, 2].set_xlabel('sqft_above')\n",
    "axs[0, 2].set_ylabel('Price')\n",
    "\n",
    "axs[1, 0].scatter(df1['sqft_basement'], df1['price'])\n",
    "axs[1, 0].set_xlabel('sqft_basement')\n",
    "axs[1, 0].set_ylabel('Price')\n",
    "\n",
    "axs[1, 1].scatter(df1['sqft_lot15'], df1['price'])\n",
    "axs[1, 1].set_xlabel('sqft_lot15')\n",
    "axs[1, 1].set_ylabel('Price')\n",
    "\n",
    "axs[1, 2].scatter(df1['sqft_living15'], df1['price'])\n",
    "axs[1, 2].set_xlabel('sqft_living15')\n",
    "axs[1, 2].set_ylabel('Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "sns.histplot(data=df1, x='sqft_living', ax=axs[0, 0])\n",
    "axs[0, 0].set_xlabel('sqft_living')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data=df1, x='sqft_lot', ax=axs[0, 1])\n",
    "axs[0, 1].set_xlabel('sqft_lot')\n",
    "axs[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data=df1, x='sqft_above', ax=axs[0, 2])\n",
    "axs[0, 2].set_xlabel('sqft_above')\n",
    "axs[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data=df1, x='sqft_basement', ax=axs[1, 0])\n",
    "axs[1, 0].set_xlabel('sqft_basement')\n",
    "axs[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data=df1, x='sqft_lot15', ax=axs[1, 1])\n",
    "axs[1, 1].set_xlabel('sqft_lot15')\n",
    "axs[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "sns.histplot(data=df1, x='sqft_living15', ax=axs[1, 2])\n",
    "axs[1, 2].set_xlabel('sqft_living15')\n",
    "axs[1, 2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"images/cont_var_hist_std3_it2\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots and histograms are now much improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "\n",
    "To improve the performance of the model and have accurate co-efficients, highly correlated variables must be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_vars = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "       'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n",
    "       'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long',\n",
    "       'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "df1_preprocessed = df1.loc[:, numeric_vars]\n",
    "df1_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df1_preprocessed, figsize=[12, 12]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a quick glance, the sqft variables seem to be highly correlated. In terms of real world application, you would expect the sqft_living to be related to sqft_basement. A new home builder would consider that the basement size be dictated by the living area or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_preprocessed.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(df1_preprocessed.corr()) > 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_corr_pairs = df1_preprocessed.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "df1_corr_pairs['pairs'] = list(zip(df1_corr_pairs.level_0, df1_corr_pairs.level_1))\n",
    "\n",
    "df1_corr_pairs.set_index(['pairs'], inplace = True)\n",
    "\n",
    "df1_corr_pairs.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# cc for correlation coefficient\n",
    "df1_corr_pairs.columns = ['cc']\n",
    "\n",
    "df1_corr_pairs.drop_duplicates(inplace=True)\n",
    "\n",
    "df1_corr_pairs[(df1_corr_pairs.cc>.75) & (df1_corr_pairs.cc<1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling the continuous variables that were found not to have a strong linear relationship with price\n",
    "\n",
    "* sqft_lot\n",
    "* lat\n",
    "* long\n",
    "* sqft_lot15\n",
    "* sqft_basement\n",
    "\n",
    "And the continuous variables with a strong linear relationship\n",
    "\n",
    "* sqft_living\n",
    "* sqft_above\n",
    "* sqft_living15\n",
    "\n",
    "In order to prevent multicollinearity, I will remove variables sqft_lot15 and sqft_above. Although sqft_above has a strong linear relationship with price, I value sqft_living and sqft_living15 as a better variable for affecting price. Generally speaking, the larger the area for living, the higher the price of a house. With surrounding houses having a larger area for living, generally the house price for the area is also high.\n",
    "\n",
    "I will also drop the variables that do not have a strong linear relationship with price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transformations\n",
    "\n",
    "By performing and multicollinearity and using the observations in iteration 1, the continuous variables to be log transformed is finalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = ['sqft_living', 'sqft_living15','price']\n",
    "\n",
    "# Log transform and normalize\n",
    "df1_cont = df1[continuous]\n",
    "\n",
    "# log features\n",
    "log_names = [f'{column}_log' for column in df1_cont.columns]\n",
    "\n",
    "df1_log = np.log(df1_cont)\n",
    "df1_log.columns = log_names\n",
    "\n",
    "# normalize (subract mean and divide by std)\n",
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "df1_log_norm = df1_log.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying categorical variables\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "axs[0, 0].scatter(df1['grade'], df1['price'])\n",
    "axs[0, 0].set_xlabel('grade')\n",
    "axs[0, 0].set_ylabel('Price')\n",
    "\n",
    "axs[0, 1].scatter(df1['bathrooms'], df1['price'])\n",
    "axs[0, 1].set_xlabel('bathrooms')\n",
    "axs[0, 1].set_ylabel('Price')\n",
    "\n",
    "axs[0, 2].scatter(df1['view'], df1['price'])\n",
    "axs[0, 2].set_xlabel('view')\n",
    "axs[0, 2].set_ylabel('Price')\n",
    "\n",
    "axs[1, 0].scatter(df1['bedrooms'], df1['price'])\n",
    "axs[1, 0].set_xlabel('bedrooms')\n",
    "axs[1, 0].set_ylabel('Price')\n",
    "\n",
    "axs[1, 1].scatter(df1['floors'], df1['price'])\n",
    "axs[1, 1].set_xlabel('floors')\n",
    "axs[1, 1].set_ylabel('Price')\n",
    "\n",
    "axs[1, 2].scatter(df1['waterfront'], df1['price'])\n",
    "axs[1, 2].set_xlabel('waterfront')\n",
    "axs[1, 2].set_ylabel('Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['grade', 'bathrooms', 'view', 'bedrooms', 'floors', 'waterfront']\n",
    "\n",
    "# Convert columns to category data type\n",
    "for col in categoricals:\n",
    "    df1[col] = df1[col].astype('category')\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1_cat = pd.get_dummies(df1[categoricals], prefix=categoricals, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df1_log_norm, df1_cat], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop('price_log', axis=1)\n",
    "y = df2['price_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_int = sm.add_constant(X)\n",
    "model = sm.OLS(y,X_int).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which variables strongly correlate with price\n",
    "\n",
    "price_corr = df2.corr()[['price_log']].sort_values(by='price_log', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(4, 20))\n",
    "heatmap = sns.heatmap(price_corr, annot=True, cmap='mako')\n",
    "heatmap.set_title('Variables Correlating with Price', fontsize=14);\n",
    "plt.savefig(\"images/df2_price_corr\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2 Model Comments\n",
    "\n",
    "Unexpectedly, the adjusted R-squared values decreased. By only choosing the continuous and categorical variables that have a strong relationship in the model, I expected an increase in the adjusted R-squared value. However the cause of the decrease might be explained by removing sqft_above. It showed high correlation with price but also high correlation with sqft_living. By removing sqft_above because of multicollinearity, the iteration 2 model is a more reliable representation of the data.\n",
    "\n",
    "Reviewing the P-value, there are some variables that are not statistically significant. I will remove those variables to further refine in iteration 3. Reviewing the co-efficients, there are some independent variables that large values and match up with the correlation heat map like grade 9, 10 and 11. However grade 12 and 13 with big co-efficients fall lower on the correlation heat map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions and KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(data=df2['price_log'], kde=True, ax=axs[0])\n",
    "axs[0].set_xlabel('price_log')\n",
    "\n",
    "sns.histplot(data=df2['sqft_living_log'], kde=True, ax=axs[1])\n",
    "axs[1].set_xlabel('sqft_living_log')\n",
    "\n",
    "sns.histplot(data=df2['sqft_living15_log'], kde=True, ax=axs[2])\n",
    "axs[2].set_xlabel('sqft_living15_log')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"images/dist_it2\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Linearity assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.scatterplot(data=df2, x='price_log', y='price_log', ax=axs[0])\n",
    "axs[0].set_xlabel('price_log')\n",
    "axs[0].set_ylabel('price_log')\n",
    "\n",
    "sns.scatterplot(data=df2, x='sqft_living_log', y='price_log', ax=axs[1])\n",
    "axs[1].set_xlabel('sqft_living_log')\n",
    "axs[1].set_ylabel('price_log')\n",
    "\n",
    "sns.scatterplot(data=df2, x='sqft_living15_log', y='price_log', ax=axs[2])\n",
    "axs[2].set_xlabel('sqft_living15_log')\n",
    "axs[2].set_ylabel('price_log')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"images/scat_lin_it2\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Normality and Homoscedasticity assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df2\n",
    "f = 'price_log~sqft_living_log'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_living_log': [data.sqft_living_log.min(), data.sqft_living_log.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_living_log', y='price_log', ax=ax)\n",
    "ax.plot(X_new['sqft_living_log'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_living_log\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "plt.savefig(\"images/price_log_sqft_living_log_norm_homo_it2\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df2\n",
    "f = 'price_log~sqft_living15_log'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_living15_log': [data.sqft_living15_log.min(), data.sqft_living15_log.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_living15_log', y='price_log', ax=ax)\n",
    "ax.plot(X_new['sqft_living15_log'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_living15_log\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "plt.savefig(\"images/price_log_sqft_living15_log_norm_homo_it2\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Assumptions Comments\n",
    "\n",
    "The normal distributions for price, sqft_living and sqft_lving15 benefitted from being log transformed. They are no longer positively skewed and follow a typical bell shaped curve making it ideal for use in linear regression.\n",
    "\n",
    "All continuous variables follow a linear trend validating the linear assumption. Performing a polynomial regression is not recommended as it could result in the model overfitting. \n",
    "\n",
    "Both sqft_living and sqft_lving15 no longer violate the normality and homoscedasticity assumptions however there are some observations to note:\n",
    "* The sqft_living R-squared value went down. This could be the result of outliers still being present before log transformation. As seen in the Q-Q plot, the tails are curved at the ends suggesting outliers in the data\n",
    "* The sqft_living15 went up after log transformation which is expected however, like the sqft_living variable, the Q-Q plot suggests outliers are still present in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 3\n",
    "\n",
    "For iteration 3, I will start by removing the outliers by reducing the data to within 2 standard deviations then observe if there has been an improvement in the R-squared value and error terms\n",
    "\n",
    "Performing a polynomial regression may result in the model overfitting so in a final effort to increase the model's adjusted R-squared value, I will perform interactions. Iteration 3 is a good step to perform interactions because I have chosen the continuous and categorical variables that have a strong relationship with price.\n",
    "\n",
    "I expect to see an increase in the adjusted R-squared compared to the iteration 2 model. Based on the observations of the linear assumptions, I will be able to perform model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers\n",
    "\n",
    "Reducing the data from 3 standard deviations to 2 in order to further remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check this is the data cleaned dataframe\n",
    "\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce outliers by reducing data size to within 2 standard deviations\n",
    "\n",
    "filter_cols = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_lot15', 'sqft_living15']\n",
    "\n",
    "df3 = df[~df[filter_cols].apply(lambda x: np.abs(x - x.mean()) > 2 * x.std()).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check entries\n",
    "\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = ['sqft_living', 'sqft_living15', 'price']\n",
    "\n",
    "# Log transform\n",
    "df3_log = np.log(df3[continuous])\n",
    "df3_log.columns = [f'{column}_log' for column in df3_log.columns]\n",
    "\n",
    "# Normalize\n",
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "df3_log_norm = df3_log.apply(normalize)\n",
    "\n",
    "categoricals = ['grade', 'bathrooms', 'view', 'bedrooms', 'floors', 'waterfront']\n",
    "\n",
    "# Convert columns to category data type\n",
    "for col in categoricals:\n",
    "    df3[col] = df[col].astype('category')\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df3_cat = pd.get_dummies(df3[categoricals], prefix=categoricals, drop_first=True)\n",
    "\n",
    "df4 = pd.concat([df3_log_norm, df3_cat], axis=1)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking model before performing interactions\n",
    "\n",
    "X = df4.drop('price_log', axis=1)\n",
    "y = df4['price_log']\n",
    "\n",
    "X_int = sm.add_constant(X)\n",
    "model = sm.OLS(y,X_int).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting to note that reducing data to within 2 standard deviations instead of 3 standard drastically reduced the adjusted R-squared value. I will need to check the linearity assumptions to see how the data is distributed at 2 standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions and KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(data=df4['price_log'], kde=True, ax=axs[0])\n",
    "axs[0].set_xlabel('price_log')\n",
    "\n",
    "sns.histplot(data=df4['sqft_living_log'], kde=True, ax=axs[1])\n",
    "axs[1].set_xlabel('sqft_living_log')\n",
    "\n",
    "sns.histplot(data=df4['sqft_living15_log'], kde=True, ax=axs[2])\n",
    "axs[2].set_xlabel('sqft_living15_log')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"images/dist_std2_it3\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Linearity assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.scatterplot(data=df4, x='price_log', y='price_log', ax=axs[0])\n",
    "axs[0].set_xlabel('price_log')\n",
    "axs[0].set_ylabel('price_log')\n",
    "\n",
    "sns.scatterplot(data=df4, x='sqft_living_log', y='price_log', ax=axs[1])\n",
    "axs[1].set_xlabel('sqft_living_log')\n",
    "axs[1].set_ylabel('price_log')\n",
    "\n",
    "sns.scatterplot(data=df4, x='sqft_living15_log', y='price_log', ax=axs[2])\n",
    "axs[2].set_xlabel('sqft_living15_log')\n",
    "axs[2].set_ylabel('price_log')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"images/scat_lin_std2_it3\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Normality and Homoscedasticity assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df4\n",
    "f = 'price_log~sqft_living_log'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_living_log': [data.sqft_living_log.min(), data.sqft_living_log.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_living_log', y='price_log', ax=ax)\n",
    "ax.plot(X_new['sqft_living_log'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_living_log\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "plt.savefig(\"images/price_log_sqft_living_std2_it3\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df4\n",
    "f = 'price_log~sqft_living15_log'\n",
    "model = smf.ols(formula=f, data=data).fit()\n",
    "print ('R-Squared:',model.rsquared)\n",
    "print (model.params)\n",
    "\n",
    "X_new = pd.DataFrame({'sqft_living15_log': [data.sqft_living15_log.min(), data.sqft_living15_log.max()]})\n",
    "preds = model.predict(X_new)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data.plot(kind='scatter', x='sqft_living15_log', y='price_log', ax=ax)\n",
    "ax.plot(X_new['sqft_living15_log'], preds, c='red', linewidth=2)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "fig = sm.graphics.plot_regress_exog(model, \"sqft_living15_log\", fig=fig)\n",
    "plt.show()\n",
    "\n",
    "residuals = model.resid\n",
    "fig = sm.graphics.qqplot(residuals, dist=stats.norm, line='45', fit=True)\n",
    "\n",
    "plt.savefig(\"images/price_log_sqft_living15_std2_it3\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 3 comments within 2 standard deviations\n",
    "\n",
    "Distributions remain normal however R-squared values have drastically reduced with data being limited to 2 standard deviations. Looking at the Q-Q plots, the top end values have resolved better however the scatter plot abruptly ends. The lower end is still experiencing negative skewness. It is evident that the outliers removed may not actually be outliers. While the 'outliers' sit far from the marjority of the data, the iteration 2 model suggests they still represent a strong correlation with price.\n",
    "\n",
    "Going forward, I will keep the data at 3 standard deviations and perform interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check this is the data cleaned dataframe\n",
    "\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating iteration 2 model as preparation for interactions and model validation\n",
    "\n",
    "filter_cols = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_lot15', 'sqft_living15']\n",
    "\n",
    "df3 = df[~df[filter_cols].apply(lambda x: np.abs(x - x.mean()) > 3 * x.std()).any(axis=1)]\n",
    "\n",
    "continuous = ['sqft_living', 'sqft_living15','price']\n",
    "\n",
    "# Log transform and normalize\n",
    "df3_cont = df3[continuous]\n",
    "\n",
    "# log features\n",
    "log_names = [f'{column}_log' for column in df3_cont.columns]\n",
    "\n",
    "df3_log = np.log(df3_cont)\n",
    "df3_log.columns = log_names\n",
    "\n",
    "# normalize (subract mean and divide by std)\n",
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "df3_log_norm = df3_log.apply(normalize)\n",
    "\n",
    "categoricals = ['grade', 'bathrooms', 'view', 'bedrooms', 'floors', 'waterfront']\n",
    "\n",
    "# Convert columns to category data type\n",
    "for col in categoricals:\n",
    "    df3[col] = df[col].astype('category')\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df3_cat = pd.get_dummies(df3[categoricals], prefix=categoricals, drop_first=True)\n",
    "\n",
    "df4 = pd.concat([df3_log_norm, df3_cat], axis=1)\n",
    "\n",
    "X = df4.drop('price_log', axis=1)\n",
    "y = df4['price_log']\n",
    "\n",
    "X_int = sm.add_constant(X)\n",
    "model = sm.OLS(y,X_int).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Interactions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "\n",
    "crossvalidation = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "baseline = np.mean(cross_val_score(regression, X, y, scoring=\"r2\", cv=crossvalidation))\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "interactions = []\n",
    "\n",
    "feat_combinations = combinations(X.columns, 2)\n",
    "\n",
    "data = X.copy()\n",
    "for i, (a, b) in enumerate(feat_combinations):\n",
    "    data[\"interaction\"] = data[a] * data[b]\n",
    "    score = np.mean(\n",
    "        cross_val_score(regression, data, y, scoring=\"r2\", cv=crossvalidation)\n",
    "    )\n",
    "    if score > baseline:\n",
    "        interactions.append((a, b, round(score, 3)))\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(\n",
    "    \"Top 3 interactions: %s\"\n",
    "    % sorted(interactions, key=lambda inter: inter[2], reverse=True)[:3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqft_living_log has a relationship with 3 separate independent variables. This suggests that where other independent variables perform well, so will sqft_living. Therefore sqft_living is an essential variable to this model\n",
    "\n",
    "Reviewing the iteration 2 model, grade_4 and bedrooms_2 are statistically insignificant. The interaction of sqft_living_log and waterfront_1.0 will be used. In terms of real world application, this interactions suggests houses with waterfront tend to be larger in square feet and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "crossvalidation = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "final = X.copy()\n",
    "\n",
    "final[\"sqft_living_log*waterfront_1.0\"] = (\n",
    "    final[\"sqft_living_log\"] * final[\"waterfront_1.0\"]\n",
    ")\n",
    "\n",
    "final_model = np.mean(\n",
    "    cross_val_score(regression, final, y, scoring=\"r2\", cv=crossvalidation)\n",
    ")\n",
    "\n",
    "print(\"Baseline Model:\" )\n",
    "print(baseline)\n",
    "print(\"Baseline Model plus interaction:\" )\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding interaction to model\n",
    "\n",
    "df_inter_sm = sm.add_constant(final)\n",
    "model = sm.OLS(y, final)\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the interaction did not increase the adjusted R-squared value. Since the model did not improve, it is most likely at its best fit and is ready for model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df4.drop('price_log', axis=1)\n",
    "y = df4['price_log']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = linreg.predict(X_train)\n",
    "y_hat_test = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_hat_train)\n",
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "print('Train Mean Squared Error:', train_mse)\n",
    "print('Test Mean Squared Error: ', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation Comments\n",
    "\n",
    "A huge case of overfitting. While the trainMSE is relatively low, the testMSE is through the roof indicating there is still a lot of variance in the data set being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 4\n",
    "\n",
    "Another iteration is required because the model is dramatically overfitting. It cannot be trusted that the data set is a true representation of the relationship between the independent variables and the target variable, price. \n",
    "\n",
    "Based on iteration 3 I will drop these indepdendent variables as they are statistically insignificant\n",
    "\n",
    "grade_4, grade_5, grade_6, grade_7, grade_8, grade_9, grade_10\n",
    "\n",
    "bathrooms_0.75, bathrooms_1.0, bathrooms_1.25, bathrooms_1.5, bathrooms_1.75, bathrooms_2.0, bathrooms_2.25, bathrooms_2.5, bathrooms_2.75, bathrooms_3.0, bathrooms_5.0, bathrooms_5.75, bathrooms_7.5, bathrooms_6.0\n",
    "\n",
    "bedrooms_8, bedrooms_9, bedrooms_10, bedrooms_11\n",
    "\n",
    "floors_3.5\n",
    "\n",
    "As an early observation only the highest of grades have a relationship with price.\n",
    "\n",
    "The house requires more than 3 bathrooms before it starts having an effect on price.\n",
    "\n",
    "The effect of bedrooms on price peaks at 8.\n",
    "\n",
    "More than 3 floors has no relevance however that contradicts with sqft_living having a strong correlation with price. The data points for houses with more than 3.5 must be outliers. That would be believable in a real world application because it is rare to see houses with more than 3 floors.\n",
    "\n",
    "Also it was observed in iteration 3 that when the data was reduced to within two standard deviations that the R-squared was reduced. Initially this was seen as negative but as we can see in the model validation, there is a lot of variance still in the data. For iteration 4, I will reduce the data to within two standard deviations. \n",
    "\n",
    "Luckily, most of these are categorical variables. Since the linear assumptions have been proven in iteration 3, they will not needed to be checked and I can focus on the model and model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_cols = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_lot15', 'sqft_living15']\n",
    "\n",
    "df3 = df[~df[filter_cols].apply(lambda x: np.abs(x - x.mean()) > 2 * x.std()).any(axis=1)]\n",
    "\n",
    "continuous = ['sqft_living', 'sqft_living15','price']\n",
    "\n",
    "# Log transform and normalize\n",
    "df3_cont = df3[continuous]\n",
    "\n",
    "# log features\n",
    "log_names = [f'{column}_log' for column in df3_cont.columns]\n",
    "\n",
    "df3_log = np.log(df3_cont)\n",
    "df3_log.columns = log_names\n",
    "\n",
    "# normalize (subract mean and divide by std)\n",
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "df3_log_norm = df3_log.apply(normalize)\n",
    "\n",
    "categoricals = ['grade', 'bathrooms', 'view', 'bedrooms', 'floors', 'waterfront']\n",
    "\n",
    "# Convert columns to category data type\n",
    "for col in categoricals:\n",
    "    df3[col] = df[col].astype('category')\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df3_cat = pd.get_dummies(df3[categoricals], prefix=categoricals, drop_first=True)\n",
    "\n",
    "df4 = pd.concat([df3_log_norm, df3_cat], axis=1)\n",
    "\n",
    "# Drop indepdendent variables\n",
    "drop_grade = ['grade_4', 'grade_5', 'grade_6', 'grade_7', 'grade_8','grade_9','grade_10']\n",
    "drop_bathrooms = ['bathrooms_0.75', 'bathrooms_1.0', 'bathrooms_1.25', 'bathrooms_1.5', 'bathrooms_1.75', 'bathrooms_2.0', 'bathrooms_2.25', 'bathrooms_2.5', 'bathrooms_2.75', 'bathrooms_3.0', 'bathrooms_5.0', 'bathrooms_5.75', 'bathrooms_7.5', 'bathrooms_6.0']\n",
    "drop_bedrooms = ['bedrooms_8', 'bedrooms_9', 'bedrooms_10', 'bedrooms_11']\n",
    "drop_floors = ['floors_3.5']\n",
    "\n",
    "X = df4.drop(['price_log'] + drop_grade + drop_bathrooms + drop_bedrooms + drop_floors, axis=1)\n",
    "y = df4['price_log']\n",
    "\n",
    "X_int = sm.add_constant(X)\n",
    "model = sm.OLS(y,X_int).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See highest to lowest co-efficients\n",
    "\n",
    "# Get the summary table as HTML\n",
    "summary_html = model.summary().tables[1].as_html()\n",
    "\n",
    "# Convert the HTML table to a DataFrame\n",
    "coef_df = pd.read_html(summary_html, header=0, index_col=0)[0]\n",
    "\n",
    "# Convert coefficient values to numeric type\n",
    "coef_df['coef'] = pd.to_numeric(coef_df['coef'], errors='coerce')\n",
    "\n",
    "# Sort the coefficients by value in descending order\n",
    "coef_df_sorted = coef_df.sort_values('coef', ascending=False)\n",
    "\n",
    "# Print the sorted coefficient table\n",
    "print(coef_df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 4 Comments\n",
    "\n",
    "Again the adjusted R-squared value has lowered as more outliers are removed, therefore reducing the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "drop_grade = ['grade_4', 'grade_5', 'grade_6', 'grade_7', 'grade_8','grade_9','grade_10']\n",
    "drop_bathrooms = ['bathrooms_0.75', 'bathrooms_1.0', 'bathrooms_1.25', 'bathrooms_1.5', 'bathrooms_1.75', 'bathrooms_2.0', 'bathrooms_2.25', 'bathrooms_2.5', 'bathrooms_2.75', 'bathrooms_3.0', 'bathrooms_5.0', 'bathrooms_5.75', 'bathrooms_7.5', 'bathrooms_6.0']\n",
    "drop_bedrooms = ['bedrooms_8', 'bedrooms_9', 'bedrooms_10', 'bedrooms_11']\n",
    "drop_floors = ['floors_3.5']\n",
    "\n",
    "X = df4.drop(['price_log'] + drop_grade + drop_bathrooms + drop_bedrooms + drop_floors, axis=1)\n",
    "y = df4['price_log']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "LinearRegression()\n",
    "\n",
    "y_hat_train = linreg.predict(X_train)\n",
    "y_hat_test = linreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_hat_train)\n",
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "print('Train Mean Squared Error:', train_mse)\n",
    "print('Test Mean Squared Error: ', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly higher errors in trainMSE however the testMSE has greatly reduced leading to a model that fits much better. It is clear the effect of removing more variance by reducing the data set within 2 standard deviations and removing statistically insignificant variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using K-Fold Cross Validation to verify\n",
    "\n",
    "drop_grade = ['grade_4', 'grade_5', 'grade_6', 'grade_7', 'grade_8','grade_9','grade_10']\n",
    "drop_bathrooms = ['bathrooms_0.75', 'bathrooms_1.0', 'bathrooms_1.25', 'bathrooms_1.5', 'bathrooms_1.75', 'bathrooms_2.0', 'bathrooms_2.25', 'bathrooms_2.5', 'bathrooms_2.75', 'bathrooms_3.0', 'bathrooms_5.0', 'bathrooms_5.75', 'bathrooms_7.5', 'bathrooms_6.0']\n",
    "drop_bedrooms = ['bedrooms_8', 'bedrooms_9', 'bedrooms_10', 'bedrooms_11']\n",
    "drop_floors = ['floors_3.5']\n",
    "\n",
    "X = df4.drop(['price_log'] + drop_grade + drop_bathrooms + drop_bedrooms + drop_floors, axis=1)\n",
    "y = df4['price_log']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "LinearRegression()\n",
    "\n",
    "y_hat_train = linreg.predict(X_train)\n",
    "y_hat_test = linreg.predict(X_test)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "scoring = {'mse': 'neg_mean_squared_error'}\n",
    "results = cross_validate(linreg, X, y, scoring=scoring, cv=10)\n",
    "\n",
    "test_mse_scores = -results['test_mse']\n",
    "\n",
    "train_predictions = linreg.predict(X)\n",
    "train_mse = mean_squared_error(y, train_predictions)\n",
    "\n",
    "print(\"Train Mean Squared Error:\", train_mse)\n",
    "print(\"Test Mean Squared Error:\", test_mse_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 variables\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "selector = RFE(linreg, n_features_to_select=5)\n",
    "selector = selector.fit(X_train, y_train.values)\n",
    "\n",
    "selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = X_train.columns[selector.support_]\n",
    "linreg.fit(X_train[selected_columns], y_train)\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "After 4 iterations, a collection of the top independent variables were used to create the final model. Using sklearn's feature selector the top 5 variables that have a strong relationship with the target variable price are:\n",
    "\n",
    "* grade_11\n",
    "* grade_12\n",
    "* bathrooms_3.75\n",
    "* view_3.0\n",
    "* view_4.0\n",
    "\n",
    "I believe this linear regression model successfully solves the business problem of identifying the five factors that have a strong relationship with price. Initially I expected the adjusted R-squared of the model to increase as more statistically significant variables were identified. However, reducing variance and outliers proved to be the more important factor which result in a lower adjusted R-squared value. \n",
    "\n",
    "Reducing the data set to within 2 standard variations initially looked extreme as the adjusted R-squared dipped to almost half of the baseline model. However when the first model validation was performed, it showed that the model was dramatically overfitting indicating a lot of variance was still present in the data set. \n",
    "\n",
    "During each iteration, the P-values held true with the correlation heat map. This provides me with great confidence that the correct variables were being chosen to be used in the model. It is important for data to also make sense in a real world application. Looking at the top 5 variables, they are realistic in the sense that a typical person would expect those variables when evaluating the price of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "King County has a ranking system that represents the construction quality of improvements. They are generally defined as:\n",
    "\n",
    "**1-3** Falls short of minimum building standards. Normally cabin or inferior structure.\n",
    "\n",
    "**4** Generally older, low quality construction. Does not meet code.\n",
    "\n",
    "**5** Low construction costs and workmanship. Small, simple design.\n",
    "\n",
    "**6** Lowest grade currently meeting building code. Low quality materials and simple designs.\n",
    "\n",
    "**7** Average grade of construction and design. Commonly seen in plats and older sub-divisions.\n",
    "\n",
    "**8** Just above average in construction and design. Usually better materials in both the exterior and interior finish work.\n",
    "\n",
    "**9** Better architectural design with extra interior and exterior design and quality.\n",
    "\n",
    "**10** Homes of this quality generally have high quality features. Finish work is better and more design quality is seen in the floor plans. Generally have a larger square footage.\n",
    "\n",
    "**11** Custom design and higher quality finish work with added amenities of solid woods, bathroom fixtures and more luxurious options.\n",
    "\n",
    "**12** Custom design and excellent builders. All materials are of the highest quality and all conveniences are present.\n",
    "\n",
    "**13** Generally custom designed and built. Mansion level. Large amount of highest quality cabinet work, wood trim, marble, entry ways etc.\n",
    "\n",
    "The results Grade 11 and 12 sit on the higher end of the ranking system and is reflective of the model that has been produced. Investing in an excellent builder, high quality materials and luxurious options will yield a higher house price.\n",
    "\n",
    "Based on the statistical significance of the number of bathrooms throughout the iterations, it was observed that the number of bathrooms only started having a strong relationship with price at 3. Again this is reflective in the results at 3.75. The results suggest that many of the higher priced homes have a minimum 3 bathrooms.\n",
    "\n",
    "The features of the home are not the only important factors in raising the price of a house. The location is just as important and in this case, if the house has a view. King County has a variety of landmarks, ocean, lake and views. Opting to build a house within view of these natural and manmade points of interest and it will have a positive effect on the price.\n",
    "\n",
    "In conclusion, in order for the east coast residential builder to be successful on the east coast, specifically King County, they must consider:\n",
    "\n",
    "* Creating a custom design using high quality materials, high quality finish work and luxurious options\n",
    "* Incorporating 3 or more bathrooms into their designs\n",
    "* Choosing a location of the house with a great view of local points of interest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
